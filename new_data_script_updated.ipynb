{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fad9af-23ca-41a3-a7fc-8298aa2b6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional bounds\n",
    "REGION_BOUNDS = {\n",
    "    'lat_min': 42.875,\n",
    "    'lat_max': 43.375,\n",
    "    'lon_min': -1.125,\n",
    "    'lon_max': 0.625\n",
    "}\n",
    "\n",
    "def download_regional_subset(\n",
    "    model,\n",
    "    scenario,\n",
    "    year,\n",
    "    output_dir,\n",
    "    variable='pr',\n",
    "    variant='r1i1p1f2',\n",
    "    grid='gr',\n",
    "    bounds=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Download regional subset from NASA THREDDS server.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        Model name (e.g., 'CNRM-CM6-1')\n",
    "    scenario : str\n",
    "        Scenario (e.g., 'historical', 'ssp245', 'ssp585')\n",
    "    year : int\n",
    "        Year to download\n",
    "    output_dir : str or Path\n",
    "        Directory to save downloaded files\n",
    "    variable : str\n",
    "        Variable name (default: 'pr')\n",
    "    variant : str\n",
    "        Model variant (default: 'r1i1p1f2')\n",
    "    grid : str\n",
    "        Grid type (default: 'gr')\n",
    "    bounds : dict\n",
    "        Regional bounds with keys: lat_min, lat_max, lon_min, lon_max\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Path to downloaded file\n",
    "    \"\"\"\n",
    "    \n",
    "    if bounds is None:\n",
    "        bounds = REGION_BOUNDS\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Construct filename\n",
    "    filename = f\"{variable}_day_{model}_{scenario}_{variant}_{grid}_{year}_subset.nc\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if output_path.exists():\n",
    "        print(f\"  Already exists: {filename}\")\n",
    "        return output_path\n",
    "    \n",
    "    # Construct THREDDS URL\n",
    "    base_url = \"https://ds.nccs.nasa.gov/thredds/ncss/grid\"\n",
    "    dataset_path = f\"AMES/NEX/GDDP-CMIP6/{model}/{scenario}/{variant}/{variable}\"\n",
    "    nc_file = f\"{variable}_day_{model}_{scenario}_{variant}_{grid}_{year}.nc\"\n",
    "    \n",
    "    # Query parameters\n",
    "    params = {\n",
    "        'var': variable,\n",
    "        'north': bounds['lat_max'],\n",
    "        'south': bounds['lat_min'],\n",
    "        'west': bounds['lon_min'],\n",
    "        'east': bounds['lon_max'],\n",
    "        'horizStride': 1,\n",
    "        'time_start': f'{year}-01-01T12:00:00Z',\n",
    "        'time_end': f'{year}-12-31T12:00:00Z',\n",
    "        'accept': 'netcdf4',\n",
    "        'addLatLon': 'true'\n",
    "    }\n",
    "    \n",
    "    url = f\"{base_url}/{dataset_path}/{nc_file}?{urlencode(params)}\"\n",
    "    \n",
    "    print(f\"  Downloading {year}...\")\n",
    "    \n",
    "    try:\n",
    "        # Download with progress\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 8192\n",
    "        downloaded = 0\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=block_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        percent = int(50 * downloaded / total_size)\n",
    "                        print(f\"\\r  Progress: [{'='*percent}{' '*(50-percent)}] {downloaded/1024/1024:.1f}MB\", end='')\n",
    "        \n",
    "        print(f\"\\n  ✓ Downloaded: {filename}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ✗ Error downloading {year}: {e}\")\n",
    "        if output_path.exists():\n",
    "            output_path.unlink()  # Delete partial file\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_scenario_data(\n",
    "    model,\n",
    "    scenario,\n",
    "    start_year,\n",
    "    end_year,\n",
    "    output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Download multiple years for a scenario.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        Model name\n",
    "    scenario : str\n",
    "        Scenario name\n",
    "    start_year, end_year : int\n",
    "        Year range to download\n",
    "    output_dir : str or Path\n",
    "        Directory to save files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List of successfully downloaded file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Downloading {scenario} data for {model}\")\n",
    "    print(f\"Years: {start_year}-{end_year}\")\n",
    "    print(f\"Region: Lat {REGION_BOUNDS['lat_min']}-{REGION_BOUNDS['lat_max']}, \"\n",
    "          f\"Lon {REGION_BOUNDS['lon_min']}-{REGION_BOUNDS['lon_max']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        file_path = download_regional_subset(\n",
    "            model=model,\n",
    "            scenario=scenario,\n",
    "            year=year,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        if file_path:\n",
    "            downloaded_files.append(file_path)\n",
    "    \n",
    "    print(f\"\\n✓ Downloaded {len(downloaded_files)} of {end_year - start_year + 1} files\")\n",
    "    \n",
    "    return downloaded_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b801293f-8592-4a8e-8b75-b78737dd27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CMIP6 Precipitation Extraction - Specific Grid Points\n",
      "======================================================================\n",
      "Model: CNRM-CM6-1\n",
      "Scenario: historical\n",
      "Period: 1950-2014\n",
      "\n",
      "Elevation groups:\n",
      "  high: 3 points\n",
      "  uppermid: 3 points\n",
      "  lowermid: 8 points\n",
      "  low: 8 points\n",
      "======================================================================\n",
      "Found 65 precipitation files for CNRM-CM6-1 historical\n",
      "\n",
      "Version summary:\n",
      "  v2.0: 65 files\n",
      "\n",
      "Processing 65 files from 1950 to 2014\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1950_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1951_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1952_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1953_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1954_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1955_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1956_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1957_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1958_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1959_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1960_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1961_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1962_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1963_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1964_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1965_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1966_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1967_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1968_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1969_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1970_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1971_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1972_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1973_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1974_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1975_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1976_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1977_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1978_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1979_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1980_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1981_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1982_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1983_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1984_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1985_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1986_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1987_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1988_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1989_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1990_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1991_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1992_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1993_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1994_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1995_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1996_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1997_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1998_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_1999_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2000_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2001_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2002_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2003_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2004_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2005_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2006_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2007_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2008_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2009_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2010_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2011_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2012_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2013_v2.0.nc\n",
      "Processing: pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_2014_v2.0.nc\n",
      "\n",
      "high: 780 months extracted\n",
      "\n",
      "uppermid: 780 months extracted\n",
      "\n",
      "lowermid: 780 months extracted\n",
      "\n",
      "low: 780 months extracted\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Summary Statistics (monthly precipitation sum):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "HIGH:\n",
      "  Number of months: 780\n",
      "  Date range: 1950-01-01 00:00:00 to 2014-12-01 00:00:00\n",
      "  Mean: 9.681074e-04 kg m-2 s-1\n",
      "  Min: 3.130063e-05 kg m-2 s-1\n",
      "  Max: 2.744363e-03 kg m-2 s-1\n",
      "\n",
      "UPPERMID:\n",
      "  Number of months: 780\n",
      "  Date range: 1950-01-01 00:00:00 to 2014-12-01 00:00:00\n",
      "  Mean: 9.428334e-04 kg m-2 s-1\n",
      "  Min: 3.353438e-05 kg m-2 s-1\n",
      "  Max: 2.569814e-03 kg m-2 s-1\n",
      "\n",
      "LOWERMID:\n",
      "  Number of months: 780\n",
      "  Date range: 1950-01-01 00:00:00 to 2014-12-01 00:00:00\n",
      "  Mean: 8.896687e-04 kg m-2 s-1\n",
      "  Min: 4.633458e-05 kg m-2 s-1\n",
      "  Max: 2.404887e-03 kg m-2 s-1\n",
      "\n",
      "LOW:\n",
      "  Number of months: 780\n",
      "  Date range: 1950-01-01 00:00:00 to 2014-12-01 00:00:00\n",
      "  Mean: 8.716050e-04 kg m-2 s-1\n",
      "  Min: 4.521774e-05 kg m-2 s-1\n",
      "  Max: 2.473057e-03 kg m-2 s-1\n",
      "\n",
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "Saved: pr_monthly_high.csv\n",
      "Saved: pr_monthly_uppermid.csv\n",
      "Saved: pr_monthly_lowermid.csv\n",
      "Saved: pr_monthly_low.csv\n",
      "\n",
      "Saved combined file: ./pr_monthly_all_groups.csv\n",
      "\n",
      "✓ All files saved successfully!\n",
      "\n",
      "First 10 rows of combined data:\n",
      "         time elevation_group  pr_monthly_sum  pr_mm_month\n",
      "0  1950-01-01            high        0.000487    42.097700\n",
      "1  1950-02-01            high        0.000031     2.704375\n",
      "2  1950-03-01            high        0.001151    99.403496\n",
      "3  1950-04-01            high        0.000287    24.780682\n",
      "4  1950-05-01            high        0.001463   126.438866\n",
      "5  1950-06-01            high        0.001275   110.154790\n",
      "6  1950-07-01            high        0.001645   142.130920\n",
      "7  1950-08-01            high        0.000275    23.740500\n",
      "8  1950-09-01            high        0.000803    69.350200\n",
      "9  1950-10-01            high        0.001694   146.369460\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Extract precipitation data for specific grid points from CMIP6 NEX-GDDP data.\n",
    "\n",
    "This version includes automatic downloading of regional subsets from NASA THREDDS server.\n",
    "\n",
    "Customized for:\n",
    "- Variable: Precipitation (pr)\n",
    "- Model: CNRM-CM6-1\n",
    "- Regional subset: Lat 42.875-43.375, Lon -1.125-0.625\n",
    "- Aggregation: Monthly SUM\n",
    "- 22 specific grid points in 4 elevation groups\n",
    "\"\"\"\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "\n",
    "# Define the specific grid points\n",
    "GRID_POINTS = {\n",
    "    'high': [\n",
    "        (42.875, -0.375),\n",
    "        (42.875, -0.125),\n",
    "        (42.875, 0.125),\n",
    "    ],\n",
    "    'uppermid': [\n",
    "        (42.875, -0.625),\n",
    "        (42.875, 0.375),\n",
    "        (42.875, 0.625),\n",
    "    ],\n",
    "    'lowermid': [\n",
    "        (43.125, -1.125),\n",
    "        (43.125, -0.875),\n",
    "        (43.125, -0.625),\n",
    "        (43.125, -0.375),\n",
    "        (43.125, -0.125),\n",
    "        (43.125, 0.125),\n",
    "        (43.125, 0.375),\n",
    "        (43.125, 0.625),\n",
    "    ],\n",
    "    'low': [\n",
    "        (43.375, -1.125),\n",
    "        (43.375, -0.875),\n",
    "        (43.375, -0.625),\n",
    "        (43.375, -0.375),\n",
    "        (43.375, -0.125),\n",
    "        (43.375, 0.125),\n",
    "        (43.375, 0.375),\n",
    "        (43.375, 0.625),\n",
    "    ]\n",
    "}\n",
    "\n",
    "def find_nearest_grid_index(array, value):\n",
    "    \"\"\"Find the index of the nearest value in array.\"\"\"\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx, array[idx]\n",
    "\n",
    "\n",
    "def extract_precipitation_monthly(\n",
    "    data_dir,\n",
    "    model='CNRM-CM6-1',\n",
    "    scenario='historical',\n",
    "    start_year=1950,\n",
    "    end_year=2014\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract monthly precipitation sums for specific grid points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Directory containing CMIP6 NetCDF files\n",
    "    model : str\n",
    "        Model name (default: 'CNRM-CM6-1')\n",
    "    scenario : str\n",
    "        Scenario (default: 'historical')\n",
    "    start_year, end_year : int\n",
    "        Year range to process\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with DataFrames for each elevation group\n",
    "    \"\"\"\n",
    "    \n",
    "    data_dir = Path(data_dir)\n",
    "    variable = 'pr'\n",
    "    \n",
    "    # Find matching files (both full files and subsets)\n",
    "    file_patterns = [\n",
    "        f\"{variable}_day_{model}_{scenario}_*_gr_*subset.nc\",  # Regional subsets\n",
    "        f\"{variable}_day_{model}_{scenario}_*_gr_*.nc\"         # Full files\n",
    "    ]\n",
    "    \n",
    "    all_files = []\n",
    "    for pattern in file_patterns:\n",
    "        all_files.extend(data_dir.glob(pattern))\n",
    "    \n",
    "    all_files = sorted(set(all_files))  # Remove duplicates\n",
    "    \n",
    "    if not all_files:\n",
    "        raise FileNotFoundError(f\"No files found in {data_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(all_files)} precipitation files for {model} {scenario}\")\n",
    "    \n",
    "    # Group files by year and select the newest version\n",
    "    files_by_year = {}\n",
    "    version_priority = {'v2.0': 3, 'v1.2': 2, 'v1.1': 1, 'v1.0': 0, 'subset': 4}\n",
    "    \n",
    "    for f in all_files:\n",
    "        # Extract year from filename\n",
    "        filename = f.stem\n",
    "        parts = filename.split('_')\n",
    "        \n",
    "        # Determine version\n",
    "        if 'subset' in filename:\n",
    "            version = 'subset'\n",
    "            # Year is in different position for subset files\n",
    "            year_str = parts[-2] if 'subset' in parts[-1] else parts[-1]\n",
    "        elif 'v2' in filename:\n",
    "            version = 'v2.0'\n",
    "            year_str = parts[-2]\n",
    "        elif 'v1' in filename:\n",
    "            if '_v1.2' in filename or '_v1_2' in filename:\n",
    "                version = 'v1.2'\n",
    "            elif '_v1.1' in filename or '_v1_1' in filename:\n",
    "                version = 'v1.1'\n",
    "            else:\n",
    "                version = 'v1.0'\n",
    "            year_str = parts[-2]\n",
    "        else:\n",
    "            version = 'v1.0'\n",
    "            year_str = parts[-1]\n",
    "        \n",
    "        try:\n",
    "            year = int(year_str)\n",
    "        except ValueError:\n",
    "            print(f\"  Warning: Could not parse year from {f.name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Keep only the highest version for each year\n",
    "        if year not in files_by_year:\n",
    "            files_by_year[year] = (f, version)\n",
    "        else:\n",
    "            existing_version = files_by_year[year][1]\n",
    "            if version_priority[version] > version_priority[existing_version]:\n",
    "                print(f\"  Upgrading {year}: {existing_version} -> {version}\")\n",
    "                files_by_year[year] = (f, version)\n",
    "    \n",
    "    print(f\"\\nVersion summary:\")\n",
    "    version_counts = {}\n",
    "    for year, (f, version) in files_by_year.items():\n",
    "        version_counts[version] = version_counts.get(version, 0) + 1\n",
    "    for version, count in sorted(version_counts.items()):\n",
    "        print(f\"  {version}: {count} files\")\n",
    "    \n",
    "    # Filter by year range\n",
    "    filtered_files = []\n",
    "    for year in sorted(files_by_year.keys()):\n",
    "        if start_year <= year <= end_year:\n",
    "            filtered_files.append(files_by_year[year][0])\n",
    "    \n",
    "    files = sorted(filtered_files)\n",
    "    print(f\"\\nProcessing {len(files)} files from {start_year} to {end_year}\")\n",
    "    \n",
    "    # Dictionary to store data for each elevation group\n",
    "    results = {group: [] for group in GRID_POINTS.keys()}\n",
    "    matched_cells = {group: [] for group in GRID_POINTS.keys()}\n",
    "    \n",
    "    # Process each file\n",
    "    for file_idx, file_path in enumerate(files):\n",
    "        print(f\"Processing: {file_path.name}\")\n",
    "        \n",
    "        ds = xr.open_dataset(file_path, engine='netcdf4')\n",
    "        \n",
    "        # Get lat/lon arrays from the file\n",
    "        lats = ds.lat.values\n",
    "        lons = ds.lon.values\n",
    "        \n",
    "        # Check if longitudes are in 0-360 format and convert grid points if needed\n",
    "        if lons.min() >= 0 and lons.max() > 180:\n",
    "            use_360 = True\n",
    "        else:\n",
    "            use_360 = False\n",
    "        \n",
    "        # For each elevation group\n",
    "        for group_name, points in GRID_POINTS.items():\n",
    "            \n",
    "            # Extract data for each point in this group\n",
    "            group_monthly_data = []\n",
    "            \n",
    "            for lat_target, lon_target in points:\n",
    "                # Convert longitude to 0-360 if data uses that format\n",
    "                if use_360 and lon_target < 0:\n",
    "                    lon_search = lon_target + 360\n",
    "                else:\n",
    "                    lon_search = lon_target\n",
    "                \n",
    "                # Find nearest grid cell\n",
    "                lat_idx, lat_actual = find_nearest_grid_index(lats, lat_target)\n",
    "                lon_idx, lon_actual = find_nearest_grid_index(lons, lon_search)\n",
    "                \n",
    "                # Store matched cells (only for first file to avoid duplicates)\n",
    "                if file_idx == 0:\n",
    "                    # Convert back to -180 to 180 for display if needed\n",
    "                    lon_display = lon_actual if lon_actual <= 180 else lon_actual - 360\n",
    "                    matched_cells[group_name].append(\n",
    "                        ((lat_target, lon_target), (lat_actual, lon_display))\n",
    "                    )\n",
    "                \n",
    "                # Check if we found a reasonable match (within 0.25 degrees)\n",
    "                if abs(lat_actual - lat_target) > 0.25:\n",
    "                    print(f\"  Warning: Lat {lat_target} matched to {lat_actual:.3f}\")\n",
    "                if abs(lon_actual - lon_search) > 0.25:\n",
    "                    print(f\"  Warning: Lon {lon_target} (searching {lon_search:.3f}) matched to {lon_actual:.3f}\")\n",
    "                \n",
    "                # Extract time series for this grid point\n",
    "                point_data = ds[variable].isel(lat=lat_idx, lon=lon_idx)\n",
    "                \n",
    "                # Resample to monthly sum\n",
    "                monthly = point_data.resample(time='1MS').sum()\n",
    "                \n",
    "                group_monthly_data.append(monthly)\n",
    "            \n",
    "            # Average across all points in this elevation group\n",
    "            group_mean = sum(group_monthly_data) / len(group_monthly_data)\n",
    "            results[group_name].append(group_mean)\n",
    "        \n",
    "        ds.close()\n",
    "    \n",
    "    # Combine all years for each group and convert to DataFrame\n",
    "    output_dfs = {}\n",
    "    \n",
    "    for group_name in GRID_POINTS.keys():\n",
    "        # Concatenate all years\n",
    "        combined = xr.concat(results[group_name], dim='time')\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = combined.to_dataframe(name='pr_monthly_sum').reset_index()\n",
    "        \n",
    "        # Add metadata\n",
    "        df.attrs['group'] = group_name\n",
    "        df.attrs['num_points'] = len(GRID_POINTS[group_name])\n",
    "        df.attrs['model'] = model\n",
    "        df.attrs['scenario'] = scenario\n",
    "        df.attrs['variable'] = 'pr'\n",
    "        df.attrs['units'] = 'kg m-2 s-1 (summed over days in month)'\n",
    "        \n",
    "        output_dfs[group_name] = df\n",
    "        \n",
    "        print(f\"\\n{group_name}: {len(df)} months extracted\")\n",
    "    \n",
    "    return output_dfs, matched_cells\n",
    "\n",
    "\n",
    "def save_results_by_group(results_dict, output_dir='.', matched_cells=None):\n",
    "    \"\"\"Save results to separate CSV files for each elevation group.\"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for group_name, df in results_dict.items():\n",
    "        output_file = output_dir / f\"pr_monthly_{group_name}.csv\"\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(f\"# NEX-GDDP-CMIP6 Monthly Precipitation - {group_name.upper()} elevation group\\n\")\n",
    "            f.write(f\"# Model: {df.attrs['model']}\\n\")\n",
    "            f.write(f\"# Scenario: {df.attrs['scenario']}\\n\")\n",
    "            f.write(f\"# Variable: {df.attrs['variable']}\\n\")\n",
    "            f.write(f\"# Units: {df.attrs['units']}\\n\")\n",
    "            f.write(f\"# NOTE: To convert to mm/month, multiply by 86400 * days_in_month\\n\") #i think the days part is not needed tho since its already summed into a month\n",
    "            f.write(f\"#       Example: 0.0005 kg m-2 s-1 * 86400 * 30 days = 1296 mm/month\\n\")\n",
    "            f.write(f\"# Number of grid points averaged: {df.attrs['num_points']}\\n\")\n",
    "            f.write(f\"#\\n\")\n",
    "            f.write(f\"# Target grid points and matched cells:\\n\")\n",
    "            \n",
    "            if matched_cells and group_name in matched_cells:\n",
    "                for target, actual in matched_cells[group_name]:\n",
    "                    f.write(f\"#   Target: ({target[0]}, {target[1]}) -> Matched: ({actual[0]:.3f}, {actual[1]:.3f})\\n\")\n",
    "            else:\n",
    "                for lat, lon in GRID_POINTS[group_name]:\n",
    "                    f.write(f\"#   ({lat}, {lon})\\n\")\n",
    "            f.write(f\"#\\n\")\n",
    "        \n",
    "        df.to_csv(output_file, mode='a', index=False)\n",
    "        print(f\"Saved: {output_file}\")\n",
    "\n",
    "\n",
    "def save_combined_results(results_dict, output_file, add_mm_month=True):\n",
    "    \"\"\"Save all elevation groups to a single CSV with group identifier.\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for group_name, df in results_dict.items():\n",
    "        df_copy = df.copy()\n",
    "        df_copy['elevation_group'] = group_name\n",
    "        \n",
    "        # Add mm/month conversion if requested\n",
    "        if add_mm_month:\n",
    "            df_copy['time'] = pd.to_datetime(df_copy['time'])\n",
    "            #df_copy['days_in_month'] = df_copy['time'].dt.days_in_month\n",
    "            df_copy['pr_mm_month'] = df_copy['pr_monthly_sum'] * 86400 #* df_copy['days_in_month']\n",
    "        \n",
    "        all_data.append(df_copy)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    if add_mm_month:\n",
    "        cols = ['time', 'elevation_group', 'pr_monthly_sum', 'pr_mm_month'] #'days_in_month',\n",
    "    else:\n",
    "        cols = ['time', 'elevation_group', 'pr_monthly_sum']\n",
    "    combined_df = combined_df[cols]\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"# NEX-GDDP-CMIP6 Monthly Precipitation - All Elevation Groups\\n\")\n",
    "        f.write(f\"# Model: {results_dict['high'].attrs['model']}\\n\")\n",
    "        f.write(f\"# Scenario: {results_dict['high'].attrs['scenario']}\\n\")\n",
    "        f.write(f\"# Variable: pr (precipitation)\\n\")\n",
    "        f.write(f\"#\\n\")\n",
    "        f.write(f\"# Units:\\n\")\n",
    "        f.write(f\"#   pr_monthly_sum: kg m-2 s-1 (summed over days in month)\\n\")\n",
    "        if add_mm_month:\n",
    "            f.write(f\"#   pr_mm_month: millimeters per month (converted)\\n\")\n",
    "        f.write(f\"#\\n\")\n",
    "        f.write(f\"# Elevation groups: high, uppermid, lowermid, low\\n\")\n",
    "        f.write(f\"#\\n\")\n",
    "        f.write(\"# Grid points by group:\\n\")\n",
    "        for group_name, points in GRID_POINTS.items():\n",
    "            f.write(f\"# {group_name}: {len(points)} points\\n\")\n",
    "            for lat, lon in points:\n",
    "                f.write(f\"#   ({lat}, {lon})\\n\")\n",
    "        f.write(f\"#\\n\")\n",
    "    \n",
    "    combined_df.to_csv(output_file, mode='a', index=False)\n",
    "    print(f\"\\nSaved combined file: {output_file}\")\n",
    "\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ===== CONFIGURATION =====\n",
    "    \n",
    "    # Data directory\n",
    "    DATA_DIR = \"C:/Users/jet58062/Desktop/french_pr_hist\"\n",
    "    \n",
    "    # Model and scenario\n",
    "    MODEL = \"CNRM-CM6-1\"\n",
    "    SCENARIO = \"historical\"  # or 'ssp126', 'ssp245', 'ssp370', 'ssp585'\n",
    "    \n",
    "    # Year range\n",
    "    START_YEAR = 1950\n",
    "    END_YEAR = 2014\n",
    "    \n",
    "    # Output directory\n",
    "    OUTPUT_DIR = \".\"\n",
    "    \n",
    "    # Download options\n",
    "    DOWNLOAD_DATA = False  # Set to True to download regional subsets from THREDDS\n",
    "    \n",
    "    # ===== DOWNLOAD DATA (if enabled) =====\n",
    "    \n",
    "    if DOWNLOAD_DATA:\n",
    "        download_scenario_data(\n",
    "            model=MODEL,\n",
    "            scenario=SCENARIO,\n",
    "            start_year=START_YEAR,\n",
    "            end_year=END_YEAR,\n",
    "            output_dir=DATA_DIR\n",
    "        )\n",
    "    \n",
    "    # ===== RUN EXTRACTION =====\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CMIP6 Precipitation Extraction - Specific Grid Points\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {MODEL}\")\n",
    "    print(f\"Scenario: {SCENARIO}\")\n",
    "    print(f\"Period: {START_YEAR}-{END_YEAR}\")\n",
    "    print(f\"\\nElevation groups:\")\n",
    "    for group, points in GRID_POINTS.items():\n",
    "        print(f\"  {group}: {len(points)} points\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Extract data\n",
    "        results, matched_cells = extract_precipitation_monthly(\n",
    "            data_dir=DATA_DIR,\n",
    "            model=MODEL,\n",
    "            scenario=SCENARIO,\n",
    "            start_year=START_YEAR,\n",
    "            end_year=END_YEAR\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXTRACTION COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\nSummary Statistics (monthly precipitation sum):\")\n",
    "        print(\"-\"*70)\n",
    "        for group_name, df in results.items():\n",
    "            print(f\"\\n{group_name.upper()}:\")\n",
    "            print(f\"  Number of months: {len(df)}\")\n",
    "            print(f\"  Date range: {df['time'].min()} to {df['time'].max()}\")\n",
    "            print(f\"  Mean: {df['pr_monthly_sum'].mean():.6e} kg m-2 s-1\")\n",
    "            print(f\"  Min: {df['pr_monthly_sum'].min():.6e} kg m-2 s-1\")\n",
    "            print(f\"  Max: {df['pr_monthly_sum'].max():.6e} kg m-2 s-1\")\n",
    "        \n",
    "        # Save results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SAVING RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        save_results_by_group(results, OUTPUT_DIR, matched_cells)\n",
    "        save_combined_results(results, f\"{OUTPUT_DIR}/pr_monthly_all_groups.csv\")\n",
    "        \n",
    "        print(\"\\n✓ All files saved successfully!\")\n",
    "        \n",
    "        # Show first few rows\n",
    "        print(\"\\nFirst 10 rows of combined data:\")\n",
    "        combined_data = pd.read_csv(f\"{OUTPUT_DIR}/pr_monthly_all_groups.csv\", comment='#')\n",
    "        print(combined_data.head(10))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4d84d-a806-453f-b6f3-65a914897e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
