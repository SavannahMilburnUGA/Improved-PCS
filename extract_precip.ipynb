{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe736df2-e611-465b-bfb8-7c05c197eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Extract precipitation data for specific grid points from CMIP6 NEX-GDDP data.\n",
    "\n",
    "This script processes already-downloaded NetCDF files and extracts data for\n",
    "22 specific grid points in the Pyrenees region, aggregating daily data to\n",
    "monthly sums.\n",
    "\n",
    "Grid points organized in 4 elevation groups:\n",
    "- high: 3 points at lat 42.875\n",
    "- uppermid: 3 points at lat 42.875\n",
    "- lowermid: 8 points at lat 43.125\n",
    "- low: 8 points at lat 43.375\n",
    "\"\"\"\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Define the specific grid points\n",
    "GRID_POINTS = {\n",
    "    'high': [\n",
    "        (42.875, -0.375),\n",
    "        (42.875, -0.125),\n",
    "        (42.875, 0.125),\n",
    "    ],\n",
    "    'uppermid': [\n",
    "        (42.875, -0.625),\n",
    "        (42.875, 0.375),\n",
    "        (42.875, 0.625),\n",
    "    ],\n",
    "    'lowermid': [\n",
    "        (43.125, -1.125),\n",
    "        (43.125, -0.875),\n",
    "        (43.125, -0.625),\n",
    "        (43.125, -0.375),\n",
    "        (43.125, -0.125),\n",
    "        (43.125, 0.125),\n",
    "        (43.125, 0.375),\n",
    "        (43.125, 0.625),\n",
    "    ],\n",
    "    'low': [\n",
    "        (43.375, -1.125),\n",
    "        (43.375, -0.875),\n",
    "        (43.375, -0.625),\n",
    "        (43.375, -0.375),\n",
    "        (43.375, -0.125),\n",
    "        (43.375, 0.125),\n",
    "        (43.375, 0.375),\n",
    "        (43.375, 0.625),\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def find_nearest_grid_index(array, value):\n",
    "    \"\"\"Find the index of the nearest value in array.\"\"\"\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx, array[idx]\n",
    "\n",
    "\n",
    "def extract_precipitation_monthly(\n",
    "    data_dir,\n",
    "    model='CNRM-CM6-1',\n",
    "    scenario='historical',\n",
    "    start_year=1950,\n",
    "    end_year=2014\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract monthly precipitation sums for specific grid points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Directory containing CMIP6 NetCDF files\n",
    "    model : str\n",
    "        Model name (default: 'CNRM-CM6-1')\n",
    "    scenario : str\n",
    "        Scenario (default: 'historical')\n",
    "    start_year, end_year : int\n",
    "        Year range to process\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (output_dfs, matched_cells)\n",
    "        - output_dfs: Dictionary with DataFrames for each elevation group\n",
    "        - matched_cells: Dictionary showing which grid cells were matched\n",
    "    \"\"\"\n",
    "    \n",
    "    data_dir = Path(data_dir)\n",
    "    variable = 'pr'\n",
    "    \n",
    "    # Find matching files (both full files and regional subsets)\n",
    "    file_patterns = [\n",
    "        f\"{variable}_day_{model}_{scenario}_*_gr_*subset.nc\",  # Regional subsets\n",
    "        f\"{variable}_day_{model}_{scenario}_*_gr_*.nc\"         # Full files\n",
    "    ]\n",
    "    \n",
    "    all_files = []\n",
    "    for pattern in file_patterns:\n",
    "        all_files.extend(data_dir.glob(pattern))\n",
    "    \n",
    "    all_files = sorted(set(all_files))  # Remove duplicates\n",
    "    \n",
    "    if not all_files:\n",
    "        raise FileNotFoundError(f\"No files found in {data_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(all_files)} precipitation files for {model} {scenario}\")\n",
    "    \n",
    "    # Group files by year and select the newest version\n",
    "    files_by_year = {}\n",
    "    version_priority = {'subset': 4, 'v2.0': 3, 'v1.2': 2, 'v1.1': 1, 'v1.0': 0}\n",
    "    \n",
    "    for f in all_files:\n",
    "        # Extract year from filename\n",
    "        filename = f.stem\n",
    "        parts = filename.split('_')\n",
    "        \n",
    "        # Determine version\n",
    "        if 'subset' in filename:\n",
    "            version = 'subset'\n",
    "            # Year is in different position for subset files\n",
    "            year_str = parts[-2] if 'subset' in parts[-1] else parts[-1]\n",
    "        elif 'v2' in filename:\n",
    "            version = 'v2.0'\n",
    "            year_str = parts[-2]\n",
    "        elif 'v1' in filename:\n",
    "            if '_v1.2' in filename or '_v1_2' in filename:\n",
    "                version = 'v1.2'\n",
    "            elif '_v1.1' in filename or '_v1_1' in filename:\n",
    "                version = 'v1.1'\n",
    "            else:\n",
    "                version = 'v1.0'\n",
    "            year_str = parts[-2]\n",
    "        else:\n",
    "            version = 'v1.0'\n",
    "            year_str = parts[-1]\n",
    "        \n",
    "        try:\n",
    "            year = int(year_str)\n",
    "        except ValueError:\n",
    "            print(f\"  Warning: Could not parse year from {f.name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Keep only the highest version for each year\n",
    "        if year not in files_by_year:\n",
    "            files_by_year[year] = (f, version)\n",
    "        else:\n",
    "            existing_version = files_by_year[year][1]\n",
    "            if version_priority[version] > version_priority[existing_version]:\n",
    "                print(f\"  Upgrading {year}: {existing_version} -> {version}\")\n",
    "                files_by_year[year] = (f, version)\n",
    "    \n",
    "    print(f\"\\nVersion summary:\")\n",
    "    version_counts = {}\n",
    "    for year, (f, version) in files_by_year.items():\n",
    "        version_counts[version] = version_counts.get(version, 0) + 1\n",
    "    for version, count in sorted(version_counts.items()):\n",
    "        print(f\"  {version}: {count} files\")\n",
    "    \n",
    "    # Filter by year range\n",
    "    filtered_files = []\n",
    "    for year in sorted(files_by_year.keys()):\n",
    "        if start_year <= year <= end_year:\n",
    "            filtered_files.append(files_by_year[year][0])\n",
    "    \n",
    "    files = sorted(filtered_files)\n",
    "    print(f\"\\nProcessing {len(files)} files from {start_year} to {end_year}\")\n",
    "    \n",
    "    # Dictionary to store data for each elevation group\n",
    "    results = {group: [] for group in GRID_POINTS.keys()}\n",
    "    matched_cells = {group: [] for group in GRID_POINTS.keys()}\n",
    "    \n",
    "    # Process each file\n",
    "    for file_idx, file_path in enumerate(files):\n",
    "        print(f\"Processing: {file_path.name}\")\n",
    "        \n",
    "        ds = xr.open_dataset(file_path, engine='netcdf4')\n",
    "        \n",
    "        # Get lat/lon arrays from the file\n",
    "        lats = ds.lat.values\n",
    "        lons = ds.lon.values\n",
    "        \n",
    "        # Check if longitudes are in 0-360 format\n",
    "        if lons.min() >= 0 and lons.max() > 180:\n",
    "            use_360 = True\n",
    "        else:\n",
    "            use_360 = False\n",
    "        \n",
    "        # For each elevation group\n",
    "        for group_name, points in GRID_POINTS.items():\n",
    "            \n",
    "            # Extract data for each point in this group\n",
    "            group_monthly_data = []\n",
    "            \n",
    "            for lat_target, lon_target in points:\n",
    "                # Convert longitude to 0-360 if data uses that format\n",
    "                if use_360 and lon_target < 0:\n",
    "                    lon_search = lon_target + 360\n",
    "                else:\n",
    "                    lon_search = lon_target\n",
    "                \n",
    "                # Find nearest grid cell\n",
    "                lat_idx, lat_actual = find_nearest_grid_index(lats, lat_target)\n",
    "                lon_idx, lon_actual = find_nearest_grid_index(lons, lon_search)\n",
    "                \n",
    "                # Store matched cells (only for first file to avoid duplicates)\n",
    "                if file_idx == 0:\n",
    "                    # Convert back to -180 to 180 for display if needed\n",
    "                    lon_display = lon_actual if lon_actual <= 180 else lon_actual - 360\n",
    "                    matched_cells[group_name].append(\n",
    "                        ((lat_target, lon_target), (lat_actual, lon_display))\n",
    "                    )\n",
    "                \n",
    "                # Check if we found a reasonable match (within 0.25 degrees)\n",
    "                if abs(lat_actual - lat_target) > 0.25:\n",
    "                    print(f\"  Warning: Lat {lat_target} matched to {lat_actual:.3f}\")\n",
    "                if abs(lon_actual - lon_search) > 0.25:\n",
    "                    print(f\"  Warning: Lon {lon_target} (searching {lon_search:.3f}) matched to {lon_actual:.3f}\")\n",
    "                \n",
    "                # Extract time series for this grid point\n",
    "                point_data = ds[variable].isel(lat=lat_idx, lon=lon_idx)\n",
    "                \n",
    "                # Resample to monthly sum\n",
    "                monthly = point_data.resample(time='1MS').sum()\n",
    "                \n",
    "                group_monthly_data.append(monthly)\n",
    "            \n",
    "            # Average across all points in this elevation group\n",
    "            group_mean = sum(group_monthly_data) / len(group_monthly_data)\n",
    "            results[group_name].append(group_mean)\n",
    "        \n",
    "        ds.close()\n",
    "    \n",
    "    # Combine all years for each group and convert to DataFrame\n",
    "    output_dfs = {}\n",
    "    \n",
    "    for group_name in GRID_POINTS.keys():\n",
    "        # Concatenate all years\n",
    "        combined = xr.concat(results[group_name], dim='time')\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = combined.to_dataframe(name='pr_monthly_sum').reset_index()\n",
    "        \n",
    "        # Add metadata\n",
    "        df.attrs['group'] = group_name\n",
    "        df.attrs['num_points'] = len(GRID_POINTS[group_name])\n",
    "        df.attrs['model'] = model\n",
    "        df.attrs['scenario'] = scenario\n",
    "        df.attrs['variable'] = 'pr'\n",
    "        df.attrs['units'] = 'kg m-2 s-1 (summed over days in month)'\n",
    "        \n",
    "        output_dfs[group_name] = df\n",
    "        \n",
    "        print(f\"\\n{group_name}: {len(df)} months extracted\")\n",
    "    \n",
    "    return output_dfs, matched_cells\n",
    "\n",
    "\n",
    "def save_results_by_group(results_dict, output_dir='.', matched_cells=None):\n",
    "    \"\"\"Save results to separate CSV files for each elevation group.\"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for group_name, df in results_dict.items():\n",
    "        output_file = output_dir / f\"pr_monthly_{group_name}.csv\"\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(f\"# NEX-GDDP-CMIP6 Monthly Precipitation - {group_name.upper()} elevation group\\n\")\n",
    "            f.write(f\"# Model: {df.attrs['model']}\\n\")\n",
    "            f.write(f\"# Scenario: {df.attrs['scenario']}\\n\")\n",
    "            f.write(f\"# Variable: {df.attrs['variable']}\\n\")\n",
    "            f.write(f\"# Units: {df.attrs['units']}\\n\")\n",
    "            f.write(f\"# NOTE: To convert to mm/month, multiply by 86400\\n\")\n",
    "            f.write(f\"#       Example: 0.0005 kg m-2 s-1 * 86400 = 43.2 mm/month\\n\")\n",
    "            f.write(f\"# Number of grid points averaged: {df.attrs['num_points']}\\n\")\n",
    "            f.write(f\"#\\n\")\n",
    "            f.write(f\"# Target grid points and matched cells:\\n\")\n",
    "            \n",
    "            if matched_cells and group_name in matched_cells:\n",
    "                for target, actual in matched_cells[group_name]:\n",
    "                    f.write(f\"#   Target: ({target[0]}, {target[1]}) -> Matched: ({actual[0]:.3f}, {actual[1]:.3f})\\n\")\n",
    "            else:\n",
    "                for lat, lon in GRID_POINTS[group_name]:\n",
    "                    f.write(f\"#   ({lat}, {lon})\\n\")\n",
    "            f.write(f\"#\\n\")\n",
    "        \n",
    "        df.to_csv(output_file, mode='a', index=False)\n",
    "        print(f\"Saved: {output_file}\")\n",
    "\n",
    "\n",
    "def save_combined_results(results_dict, output_file, add_mm_month=True):\n",
    "    \"\"\"Save all elevation groups to a single CSV with group identifier.\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for group_name, df in results_dict.items():\n",
    "        df_copy = df.copy()\n",
    "        df_copy['elevation_group'] = group_name\n",
    "        \n",
    "        # Add mm/month conversion if requested\n",
    "        if add_mm_month:\n",
    "            df_copy['time'] = pd.to_datetime(df_copy['time'])\n",
    "            # Correct conversion: just multiply by seconds per day\n",
    "            # Each value is already a monthly sum of daily rates\n",
    "            df_copy['pr_mm_month'] = df_copy['pr_monthly_sum'] * 86400\n",
    "        \n",
    "        all_data.append(df_copy)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    if add_mm_month:\n",
    "        cols = ['time', 'elevation_group', 'pr_monthly_sum', 'pr_mm_month']\n",
    "    else:\n",
    "        cols = ['time', 'elevation_group', 'pr_monthly_sum']\n",
    "    combined_df = combined_df[cols]\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"# NEX-GDDP-CMIP6 Monthly Precipitation - All Elevation Groups\\n\")\n",
    "        f.write(f\"# Model: {results_dict['high'].attrs['model']}\\n\")\n",
    "        f.write(f\"# Scenario: {results_dict['high'].attrs['scenario']}\\n\")\n",
    "        f.write(f\"# Variable: pr (precipitation)\\n\")\n",
    "        f.write(f\"#\\n\")\n",
    "        f.write(f\"# Units:\\n\")\n",
    "        f.write(f\"#   pr_monthly_sum: kg m-2 s-1 (summed over days in month)\\n\")\n",
    "        if add_mm_month:\n",
    "            f.write(f\"#   pr_mm_month: millimeters per month (converted)\\n\")\n",
    "            f.write(f\"#   Conversion: pr_mm_month = pr_monthly_sum * 86400\\n\")\n",
    "        f.write(f\"#\\n\")\n",
    "        f.write(f\"# Elevation groups: high, uppermid, lowermid, low\\n\")\n",
    "        f.write(f\"#\\n\")\n",
    "        f.write(\"# Grid points by group:\\n\")\n",
    "        for group_name, points in GRID_POINTS.items():\n",
    "            f.write(f\"# {group_name}: {len(points)} points\\n\")\n",
    "            for lat, lon in points:\n",
    "                f.write(f\"#   ({lat}, {lon})\\n\")\n",
    "        f.write(f\"#\\n\")\n",
    "    \n",
    "    combined_df.to_csv(output_file, mode='a', index=False)\n",
    "    print(f\"\\nSaved combined file: {output_file}\")\n",
    "\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ===== CONFIGURATION =====\n",
    "    \n",
    "    # Data directory containing NetCDF files\n",
    "    DATA_DIR = \"C:/Users/jet58062/Desktop/french_pr_hist\"\n",
    "    \n",
    "    # Model and scenario\n",
    "    MODEL = \"CNRM-CM6-1\"\n",
    "    SCENARIO = \"historical\"  # or 'ssp126', 'ssp245', 'ssp370', 'ssp585'\n",
    "    \n",
    "    # Year range\n",
    "    START_YEAR = 1950\n",
    "    END_YEAR = 2014\n",
    "    \n",
    "    # Output directory for CSV files\n",
    "    OUTPUT_DIR = \".\"  # Current directory\n",
    "    \n",
    "    # ===== RUN EXTRACTION =====\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CMIP6 Precipitation Extraction - Specific Grid Points\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {MODEL}\")\n",
    "    print(f\"Scenario: {SCENARIO}\")\n",
    "    print(f\"Period: {START_YEAR}-{END_YEAR}\")\n",
    "    print(f\"Data directory: {DATA_DIR}\")\n",
    "    print(f\"\\nElevation groups:\")\n",
    "    for group, points in GRID_POINTS.items():\n",
    "        print(f\"  {group}: {len(points)} points\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Extract data\n",
    "        results, matched_cells = extract_precipitation_monthly(\n",
    "            data_dir=DATA_DIR,\n",
    "            model=MODEL,\n",
    "            scenario=SCENARIO,\n",
    "            start_year=START_YEAR,\n",
    "            end_year=END_YEAR\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXTRACTION COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(\"-\"*70)\n",
    "        for group_name, df in results.items():\n",
    "            print(f\"\\n{group_name.upper()}:\")\n",
    "            print(f\"  Months: {len(df)}\")\n",
    "            print(f\"  Date range: {df['time'].min()} to {df['time'].max()}\")\n",
    "            print(f\"  Mean: {df['pr_monthly_sum'].mean():.6e} kg m-2 s-1 ({df['pr_monthly_sum'].mean() * 86400:.1f} mm/month)\")\n",
    "            print(f\"  Min: {df['pr_monthly_sum'].min():.6e} kg m-2 s-1 ({df['pr_monthly_sum'].min() * 86400:.1f} mm/month)\")\n",
    "            print(f\"  Max: {df['pr_monthly_sum'].max():.6e} kg m-2 s-1 ({df['pr_monthly_sum'].max() * 86400:.1f} mm/month)\")\n",
    "        \n",
    "        # Save results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SAVING RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        save_results_by_group(results, OUTPUT_DIR, matched_cells)\n",
    "        save_combined_results(results, f\"{OUTPUT_DIR}/pr_monthly_all_groups.csv\")\n",
    "        \n",
    "        print(\"\\n✓ All files saved successfully!\")\n",
    "        \n",
    "        # Show first few rows\n",
    "        print(\"\\nFirst 10 rows of combined data:\")\n",
    "        combined_data = pd.read_csv(f\"{OUTPUT_DIR}/pr_monthly_all_groups.csv\", comment='#')\n",
    "        print(combined_data.head(10))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
